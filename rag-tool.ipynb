{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Add dotenv for environment variable loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Anthropic API key from environment variables\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not anthropic_api_key:\n",
    "    print(\"Warning: ANTHROPIC_API_KEY not found in environment variables. Make sure to set it in the .env file.\")\n",
    "\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "\n",
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def bs4_extractor(html: str, base_url=None) -> str:\n",
    "    \"\"\"\n",
    "    Extract and clean up the main content from HTML using BeautifulSoup.\n",
    "    \n",
    "    This function also follows meta-refresh redirects to ensure the latest content is fetched.\n",
    "    \n",
    "    Args:\n",
    "        html (str): The HTML content to extract text from\n",
    "        base_url (str): The base URL for resolving relative redirects (default: None)\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted and cleaned content\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Check for meta-refresh redirect\n",
    "    meta = soup.find('meta', attrs={'http-equiv': re.compile('refresh', re.I)})\n",
    "    if meta and isinstance(meta, Tag) and meta.attrs.get('content'):\n",
    "        content_value = meta.attrs.get('content')\n",
    "        # Ensure content_value is a string\n",
    "        if isinstance(content_value, str):\n",
    "            match = re.search(r'url=([^;]+)', content_value, re.IGNORECASE)\n",
    "            if match and base_url:\n",
    "                redirect_url = urljoin(base_url, match.group(1).strip())\n",
    "                print(f\"Following meta-refresh redirect to: {redirect_url}\")\n",
    "                resp = requests.get(redirect_url, timeout=15)\n",
    "                soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    \n",
    "    # Target the main article content for LangGraph documentation \n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "    \n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def load_langgraph_docs():\n",
    "    \"\"\"\n",
    "    Load LangGraph documentation from the official website.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the LangGraph website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "    # Load the documentation \n",
    "    # Appended index.html to URLs ending with / to directly access the content page\n",
    "    urls = [\n",
    "        \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/how-tos/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\",\n",
    "    ]\n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        print(f\"Attempting to load URL: {url}\")\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=15)\n",
    "            text = bs4_extractor(resp.text, base_url=url)\n",
    "            from langchain_core.documents import Document\n",
    "            doc = Document(page_content=text, metadata={\"source\": url})\n",
    "            docs.append(doc)\n",
    "            print(f\"Successfully loaded content from {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {url}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "    \n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "    \n",
    "    return docs, tokens_per_doc\n",
    "\n",
    "def save_llms_full(documents):\n",
    "    \"\"\" Save the documents to a file \"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"llms_full.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get('source', 'Unknown URL')\n",
    "            \n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "    \n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size adjusted to better match the bge-large-en-v1.5 model's context window\n",
    "    # chunk_overlap adjusted proportionally\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1500,  # ~512 tokens, matching model's context window\n",
    "        chunk_overlap=150  # Adjusted overlap (10% of chunk size)\n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "    \n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "    \n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "    \n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "        \n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "    \n",
    "    # Initialize free embeddings from SentenceTransformers\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    # You'll need to install the following packages:\n",
    "    # pip install sentence-transformers\n",
    "    # pip install langchain-community\n",
    "    \n",
    "    # Using BAAI/bge-large-en-v1.5 which can handle up to 512 tokens and provides better performance for retrieval tasks compared to all-MiniLM-L6-v2\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "    \n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd()+\"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path   ,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LangGraph documentation...\n",
      "Attempting to load URL: https://langchain-ai.github.io/langgraph/concepts/\n",
      "Following meta-refresh redirect to: https://langchain-ai.github.io/langgraph/\n",
      "Successfully loaded content from https://langchain-ai.github.io/langgraph/concepts/\n",
      "Attempting to load URL: https://langchain-ai.github.io/langgraph/how-tos/\n",
      "Following meta-refresh redirect to: https://langchain-ai.github.io/langgraph/\n",
      "Successfully loaded content from https://langchain-ai.github.io/langgraph/how-tos/\n",
      "Attempting to load URL: https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "Successfully loaded content from https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "Attempting to load URL: https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "Following meta-refresh redirect to: https://langchain-ai.github.io/langgraph/concepts/why-langgraph\n",
      "Successfully loaded content from https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "Attempting to load URL: https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Successfully loaded content from https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Loaded 5 documents from LangGraph documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://langchain-ai.github.io/langgraph/concepts/\n",
      "2. https://langchain-ai.github.io/langgraph/how-tos/\n",
      "3. https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "4. https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "5. https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Total tokens in loaded documents: 11478\n",
      "Documents concatenated into llms_full.txt\n",
      "Splitting documents...\n",
      "Created 14 chunks from documents.\n",
      "Total tokens in split documents: 12166\n",
      "Creating SKLearnVectorStore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13262/2549447892.py:213: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
      "/home/polivei/miniforge3/envs/langgraph_mcp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to /home/polivei/Desktop/ccode/langgraph-rag-mcp/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_langgraph_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "LangGraph Platform quickstart¶\n",
      "This guide shows you how to run a LangGraph application locally.\n",
      "Prerequisites¶\n",
      "Before you begin, ensure you have the following:\n",
      "\n",
      "An API key for LangSmith - free to sign up\n",
      "\n",
      "1. Install the LangGraph CLI¶\n",
      "Python serverNode server\n",
      "\n",
      "# Python >= 3.11 is required.\n",
      "\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "npx @langchain/langgraph-cli\n",
      "\n",
      "2. Create a LangGraph app 🌱¶\n",
      "Create a new app from the new-langgraph-project-python template or new-langgraph-project-js template. T\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "LangGraph Platform quickstart¶\n",
      "This guide shows you how to run a LangGraph application locally.\n",
      "Prerequisites¶\n",
      "Before you begin, ensure you have the following:\n",
      "\n",
      "An API key for LangSmith - free to sign up\n",
      "\n",
      "1. Install the LangGraph CLI¶\n",
      "Python serverNode server\n",
      "\n",
      "# Python >= 3.11 is required.\n",
      "\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "npx @langchain/langgraph-cli\n",
      "\n",
      "2. Create a LangGraph app 🌱¶\n",
      "Create a new app from the new-langgraph-project-python template or new-langgraph-project-js template. T\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "LangGraph Platform quickstart¶\n",
      "This guide shows you how to run a LangGraph application locally.\n",
      "Prerequisites¶\n",
      "Before you begin, ensure you have the following:\n",
      "\n",
      "An API key for LangSmith - free to sign up\n",
      "\n",
      "1. Install the LangGraph CLI¶\n",
      "Python serverNode server\n",
      "\n",
      "# Python >= 3.11 is required.\n",
      "\n",
      "pip install --upgrade \"langgraph-cli[inmem]\"\n",
      "\n",
      "npx @langchain/langgraph-cli\n",
      "\n",
      "2. Create a LangGraph app 🌱¶\n",
      "Create a new app from the new-langgraph-project-python template or new-langgraph-project-js template. T\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "# Get relevant documents for the query\n",
    "query = \"explain the difference between LangChain and LangGraph\"    \n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata['source'])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "@tool\n",
    "def langgraph_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the LangGraph documentation using a retriever.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "    embedding=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\"), \n",
    "    persist_path=os.getcwd()+\"/sklearn_vectorstore.parquet\", \n",
    "    serializer=\"parquet\").as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join([f\"==DOCUMENT {i+1}==\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['class langchain_anthropic.chat_models.ChatAnthropic'](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)\n",
    "\n",
    "['langchain-anthropic'](https://python.langchain.com/docs/integrations/providers/anthropic/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help explain the difference between LangChain and LangGraph. Let me search the documentation for this information.\", 'type': 'text'}, {'id': 'toolu_01GzQBW2Guw6xXqiA2UMBFC2', 'input': {'query': 'difference between LangChain and LangGraph'}, 'name': 'langgraph_query_tool', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  langgraph_query_tool (toolu_01GzQBW2Guw6xXqiA2UMBFC2)\n",
      " Call ID: toolu_01GzQBW2Guw6xXqiA2UMBFC2\n",
      "  Args:\n",
      "    query: difference between LangChain and LangGraph\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model_name=\"claude-3-7-sonnet-latest\", timeout=None, temperature=0.0, stop=None)\n",
    "augmented_llm = llm.bind_tools([langgraph_query_tool])\n",
    "\n",
    "instructions = \"\"\"You are a helpful assistant that can answer questions about the LangGraph documentation. \n",
    "Use the langgraph_query_tool for any questions about the documentation.\n",
    "If you don't know the answer, say \"I don't know.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": \"explain the difference between LangChain and LangGraph\"},\n",
    "]\n",
    "\n",
    "message = augmented_llm.invoke(messages)\n",
    "message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
